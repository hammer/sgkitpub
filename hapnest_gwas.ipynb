{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Hapnest data onto Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hapnest_ftp_base = 'ftp://ftp.ebi.ac.uk//biostudies/fire/S-BSST/936/S-BSST936/Files/example/'\n",
    "hapnest_example_files = [\n",
    "    'synthetic_small_v1_chr-10.bed',\n",
    "    'synthetic_small_v1_chr-10.bim',\n",
    "    'synthetic_small_v1_chr-10.fam',\n",
    "    'synthetic_small_v1_chr-10.sample',\n",
    "    'synthetic_small_v1_chr-11.bed',\n",
    "    'synthetic_small_v1_chr-11.bim',\n",
    "    'synthetic_small_v1_chr-11.fam',\n",
    "    'synthetic_small_v1_chr-11.sample',\n",
    "    'synthetic_small_v1_chr-12.bed',\n",
    "    'synthetic_small_v1_chr-12.bim',\n",
    "    'synthetic_small_v1_chr-12.fam',\n",
    "    'synthetic_small_v1_chr-12.sample',\n",
    "    'synthetic_small_v1_chr-13.bed',\n",
    "    'synthetic_small_v1_chr-13.bim',\n",
    "    'synthetic_small_v1_chr-13.fam',\n",
    "    'synthetic_small_v1_chr-13.sample',\n",
    "    'synthetic_small_v1_chr-14.bed',\n",
    "    'synthetic_small_v1_chr-14.bim',\n",
    "    'synthetic_small_v1_chr-14.fam',\n",
    "    'synthetic_small_v1_chr-14.sample',\n",
    "    'synthetic_small_v1_chr-15.bed',\n",
    "    'synthetic_small_v1_chr-15.bim',\n",
    "    'synthetic_small_v1_chr-15.fam',\n",
    "    'synthetic_small_v1_chr-15.sample',\n",
    "    'synthetic_small_v1_chr-16.bed',\n",
    "    'synthetic_small_v1_chr-16.bim',\n",
    "    'synthetic_small_v1_chr-16.fam',\n",
    "    'synthetic_small_v1_chr-16.sample',\n",
    "    'synthetic_small_v1_chr-17.bed',\n",
    "    'synthetic_small_v1_chr-17.bim',\n",
    "    'synthetic_small_v1_chr-17.fam',\n",
    "    'synthetic_small_v1_chr-17.sample',\n",
    "    'synthetic_small_v1_chr-18.bed',\n",
    "    'synthetic_small_v1_chr-18.bim',\n",
    "    'synthetic_small_v1_chr-18.fam',\n",
    "    'synthetic_small_v1_chr-18.sample',\n",
    "    'synthetic_small_v1_chr-19.bed',\n",
    "    'synthetic_small_v1_chr-19.bim',\n",
    "    'synthetic_small_v1_chr-19.fam',\n",
    "    'synthetic_small_v1_chr-19.sample',\n",
    "    'synthetic_small_v1_chr-1.bed',\n",
    "    'synthetic_small_v1_chr-1.bim',\n",
    "    'synthetic_small_v1_chr-1.fam',\n",
    "    'synthetic_small_v1_chr-1.sample',\n",
    "    'synthetic_small_v1_chr-20.bed',\n",
    "    'synthetic_small_v1_chr-20.bim',\n",
    "    'synthetic_small_v1_chr-20.fam',\n",
    "    'synthetic_small_v1_chr-20.sample',\n",
    "    'synthetic_small_v1_chr-21.bed',\n",
    "    'synthetic_small_v1_chr-21.bim',\n",
    "    'synthetic_small_v1_chr-21.fam',\n",
    "    'synthetic_small_v1_chr-21.sample',\n",
    "    'synthetic_small_v1_chr-22.bed',\n",
    "    'synthetic_small_v1_chr-22.bim',\n",
    "    'synthetic_small_v1_chr-22.fam',\n",
    "    'synthetic_small_v1_chr-22.sample',\n",
    "    'synthetic_small_v1_chr-2.bed',\n",
    "    'synthetic_small_v1_chr-2.bim',\n",
    "    'synthetic_small_v1_chr-2.fam',\n",
    "    'synthetic_small_v1_chr-2.sample',\n",
    "    'synthetic_small_v1_chr-3.bed',\n",
    "    'synthetic_small_v1_chr-3.bim',\n",
    "    'synthetic_small_v1_chr-3.fam',\n",
    "    'synthetic_small_v1_chr-3.sample',\n",
    "    'synthetic_small_v1_chr-4.bed',\n",
    "    'synthetic_small_v1_chr-4.bim',\n",
    "    'synthetic_small_v1_chr-4.fam',\n",
    "    'synthetic_small_v1_chr-4.sample',\n",
    "    'synthetic_small_v1_chr-5.bed',\n",
    "    'synthetic_small_v1_chr-5.bim',\n",
    "    'synthetic_small_v1_chr-5.fam',\n",
    "    'synthetic_small_v1_chr-5.sample',\n",
    "    'synthetic_small_v1_chr-6.bed',\n",
    "    'synthetic_small_v1_chr-6.bim',\n",
    "    'synthetic_small_v1_chr-6.fam',\n",
    "    'synthetic_small_v1_chr-6.sample',\n",
    "    'synthetic_small_v1_chr-7.bed',\n",
    "    'synthetic_small_v1_chr-7.bim',\n",
    "    'synthetic_small_v1_chr-7.fam',\n",
    "    'synthetic_small_v1_chr-7.sample',\n",
    "    'synthetic_small_v1_chr-8.bed',\n",
    "    'synthetic_small_v1_chr-8.bim',\n",
    "    'synthetic_small_v1_chr-8.fam',\n",
    "    'synthetic_small_v1_chr-8.sample',\n",
    "    'synthetic_small_v1_chr-9.bed',\n",
    "    'synthetic_small_v1_chr-9.bim',\n",
    "    'synthetic_small_v1_chr-9.fam',\n",
    "    'synthetic_small_v1_chr-9.sample',\n",
    "    'synthetic_small_v1.pheno1',\n",
    "    'synthetic_small_v1.pheno2',\n",
    "    'synthetic_small_v1.pheno3',\n",
    "    'synthetic_small_v1.pheno4',\n",
    "    'synthetic_small_v1.pheno5',\n",
    "    'synthetic_small_v1.pheno6',\n",
    "    'synthetic_small_v1.pheno7',\n",
    "    'synthetic_small_v1.pheno8',\n",
    "    'synthetic_small_v1.pheno9',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a connection to GCS\n",
    "project = 'sgkit-on-coiled'\n",
    "token_path = '/home/codespace/.config/gcloud/application_default_credentials.json'\n",
    "gcs = gcsfs.GCSFileSystem(project=project, token=token_path)\n",
    "#gcs.ls('hapnest/example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file over FTP to local, upload to GCS, delete local\n",
    "# Cell requires: hapnest_example_files, gcs\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "for f in hapnest_example_files:\n",
    "    print('Downloading', f)\n",
    "    urllib.request.urlretrieve(hapnest_ftp_base+f, f)\n",
    "    gcs.put(f, 'hapnest/example/'+f)\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some analysis on the Hapnest example data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An overview of the data\n",
    "- [HAPNEST BioStudies page](https://www.ebi.ac.uk/biostudies/studies/S-BSST936)\n",
    "- [PLINK File format reference](https://www.cog-genomics.org/plink/1.9/formats)\n",
    "\n",
    "- `.bed` files are binary files that contain genotype data.\n",
    "- [`.bim`](https://www.cog-genomics.org/plink/1.9/formats#bim) files are no header TSVs with information about the SNPs in the data.\n",
    "- [`.fam`](https://www.cog-genomics.org/plink/1.9/formats#fam) files are no header TSVs with information about the individuals in the data.\n",
    "- `.sample` has the ancestry group of each individual.\n",
    "- `.phenoX` files are TSVs with headers Sample, GenoEff, CovarEff, EnvEff, Phenotype(liability), Phenotype(binary). Liability is defined in the supplementary materials to be the sum of the covariate effects and the genetic effects. The binary phenotype is defined as 1 if the liability is greater than 0 and 0 otherwise (I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyData\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "xr.set_options(display_expand_attrs=False, display_expand_data_vars=True);\n",
    "\n",
    "# Google Cloud\n",
    "import gcsfs\n",
    "\n",
    "# sgkit\n",
    "import sgkit as sg\n",
    "from sgkit.io import plink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_base = 'synthetic_small_v1_chr-1'\n",
    "\n",
    "# Files to get from GCS\n",
    "files = [\n",
    "    file_base + '.bed',\n",
    "    file_base + '.bim',\n",
    "    file_base + '.fam',\n",
    "]\n",
    "\n",
    "# Local directory to put them in\n",
    "data_dir = '../data/'\n",
    "local_dir = data_dir + file_base\n",
    "\n",
    "local_plink_path = local_dir+'/'+file_base # read_plink adds .bed, .bim, .fam\n",
    "remote_zarr_path = 'gs://hapnest/example/'+file_base+'.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move PLINK files from GCS to the local fs \n",
    "for f in files:\n",
    "    print('Downloading', f)\n",
    "    gcs.get('hapnest/example/'+f, local_dir+'/'+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local PLINK files and write them as Zarr file to GCS\n",
    "chr1 = plink.read_plink(path=local_plink_path)\n",
    "chr1.to_zarr(gcs.get_mapper(remote_zarr_path), mode='w') # do I need get_mapper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sg.load_dataset(remote_zarr_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"variant_contig_name\"] = ds.contig_id[ds.variant_contig]\n",
    "ds2 = ds.set_index({\"variants\": (\"variant_contig_name\", \"variant_position\", \"variant_id\")})\n",
    "sg.display_genotypes(ds2, max_variants=10, max_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_variant = ds[[v for v in ds.data_vars if v.startswith(\"variant_\")]].to_dataframe()\n",
    "df_variant.groupby([\"variant_contig_name\", \"variant_position\", \"variant_id\"]).agg({\"variant_allele\": lambda x: list(x)}).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sample_id[:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_file = 'gs://hapnest/example/'+file_base+'.sample'\n",
    "ancestry_df = pd.read_csv(ancestry_file, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dask array from df and add to ds as a new data variable called \"sample_ancestry\" with the \"samples\" dimension\n",
    "# Is there a better dtype to use than \"object\"?\n",
    "ancestry_da = da.from_array(ancestry_df[0].values, chunks=(600,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.assign(sample_ancestry=('samples', ancestry_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: all phenotypes\n",
    "# TODO: recode binary phenotypes as bool?\n",
    "# TODO: use sample index to merge instead of just grabbing the array?\n",
    "pheno_base = 'synthetic_small_v1'\n",
    "pheno_file = 'gs://hapnest/example/'+pheno_base+'.pheno1'\n",
    "pheno_df = pd.read_csv(pheno_file, sep='\\t', index_col='Sample')\n",
    "pheno_da = da.from_array(pheno_df['Phenotype(binary)'].values, chunks=(600,))\n",
    "ds = ds.assign(sample_pheno=('samples', pheno_da))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sample_ancestry.to_series().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sample_pheno.to_series().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_variant.groupby([\"variant_contig_name\", \"variant_position\", \"variant_id\"])[\"variant_allele\"].apply(tuple).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sg.sample_stats(ds)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sample_call_rate.attrs[\"long_name\"] = \"Sample call rate\"\n",
    "xr.plot.hist(ds.sample_call_rate, range=(.88,1), bins=50, size=8, edgecolor=\"black\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sg.variant_stats(ds)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sg.hardy_weinberg_test(ds)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.sel(variants=((ds.variant_allele_frequency[:,1] > 0.01) & (ds.variant_hwe_p_value > 1e-6)).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Samples: {len(ds.samples)}  Variants: {len(ds.variants)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"call_dosage\"] = ds.call_genotype.sum(dim=\"ploidy\")\n",
    "ds_lr = sg.gwas_linear_regression(ds,\n",
    "                                  dosage=\"call_dosage\",\n",
    "                                  add_intercept=True,\n",
    "                                  covariates=[],\n",
    "                                  traits=[\"sample_pheno\"])\n",
    "ds_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_plot(ds):\n",
    "    df = ds[[\"variant_contig_name\", \"variant_contig\", \"variant_position\", \"variant_linreg_p_value\"]].to_dataframe()\n",
    "    df[\"variant_linreg_log_p_value\"] = -np.log10(df[\"variant_linreg_p_value\"])\n",
    "    df = df.astype({\"variant_position\": np.int64}) # to avoid overflow in cumulative_pos\n",
    "    \n",
    "    # from https://github.com/mojones/video_notebooks/blob/master/Manhattan%20plots%20in%20Python.ipynb, cell 20\n",
    "    running_pos = 0\n",
    "    cumulative_pos = []\n",
    "    for chrom, group_df in df.groupby(\"variant_contig\"):  \n",
    "        cumulative_pos.append(group_df[\"variant_position\"] + running_pos)\n",
    "        running_pos += group_df[\"variant_position\"].max()\n",
    "    df[\"cumulative_pos\"] = pd.concat(cumulative_pos)\n",
    "    \n",
    "    df[\"color group\"] = df[\"variant_contig\"].apply(lambda x : \"A\" if x % 2 == 0 else \"B\")\n",
    "    \n",
    "    g = sns.relplot(\n",
    "        data = df,\n",
    "        x = \"cumulative_pos\",\n",
    "        y = \"variant_linreg_log_p_value\",\n",
    "        hue = \"color group\",\n",
    "        palette = [\"blue\", \"orange\"],\n",
    "        linewidth=0,\n",
    "        s=10,\n",
    "        legend=None,\n",
    "        aspect=3\n",
    "    )\n",
    "    g.ax.set_xlabel(\"Chromosome\")\n",
    "    g.ax.set_xticks(df.groupby(\"variant_contig\")[\"cumulative_pos\"].median())\n",
    "    g.ax.set_xticklabels(df[\"variant_contig_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_plot(ds_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def qq_plot(ds):\n",
    "    p = ds[\"variant_linreg_p_value\"].squeeze().values\n",
    "    p.sort()\n",
    "    n = len(p)\n",
    "    expected_p = -np.log10(np.arange(1, n + 1) / n)\n",
    "    observed_p = -np.log10(p)\n",
    "    max_val = math.ceil(max(np.max(expected_p), np.max(observed_p)))\n",
    "\n",
    "    df = pd.DataFrame({\"Expected -log10(p)\": expected_p, \"Observed -log10(p)\": observed_p})\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 12));\n",
    "    g = sns.scatterplot(data=df, x=\"Expected -log10(p)\", y=\"Observed -log10(p)\", ax=ax, linewidth=0)\n",
    "\n",
    "    x_pred = np.linspace(0, max_val, 50)\n",
    "    sns.lineplot(x=x_pred, y=x_pred, ax=ax)\n",
    "    \n",
    "    g.set(xlim=(0, max_val), ylim=(0, max_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq_plot(ds_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pca = sg.stats.pca.count_call_alternate_alleles(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run PCA we need to filter out variants with any missing alt allele counts\n",
    "# Or where the counts are zero for all samples\n",
    "variant_mask = (((ds_pca.call_alternate_allele_count < 0).any(dim=\"samples\")) | \\\n",
    "    (ds_pca.call_alternate_allele_count.std(dim=\"samples\") <= 0.0)).compute()\n",
    "ds_pca = ds_pca.sel(variants=~variant_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pca = sg.pca(ds_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pca.sample_pca_projection.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pca[\"sample_pca_projection_0\"] = ds_pca.sample_pca_projection[:,0]\n",
    "ds_pca[\"sample_pca_projection_1\"] = ds_pca.sample_pca_projection[:,1]\n",
    "ds_pca[\"sample_pca_projection_2\"] = ds_pca.sample_pca_projection[:,2]\n",
    "# scatterplot fails: unable to allocate 420. GiB\n",
    "# TODO: fix scatterplot\n",
    "#ds_pca.plot.scatter(x=\"sample_pca_projection_0\", y=\"sample_pca_projection_1\", hue=\"sample_ancestry\", size=8, s=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy pca components back to dataset with full set of variants to run linear regression again\n",
    "ds[\"sample_pca_projection_0\"] = ds_pca.sample_pca_projection[:,0]\n",
    "ds[\"sample_pca_projection_1\"] = ds_pca.sample_pca_projection[:,1]\n",
    "ds[\"sample_pca_projection_2\"] = ds_pca.sample_pca_projection[:,2]\n",
    "ds_lr = sg.gwas_linear_regression(ds, dosage=\"call_dosage\",\n",
    "                                  add_intercept=True,\n",
    "                                  covariates=[\"sample_pca_projection_0\", \"sample_pca_projection_1\", \"sample_pca_projection_2\"],\n",
    "                                  traits=[\"sample_pheno\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq_plot(ds_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_plot(ds_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
