{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Hapnest data onto Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hapnest_ftp_base = 'ftp://ftp.ebi.ac.uk//biostudies/fire/S-BSST/936/S-BSST936/Files/example/'\n",
    "hapnest_example_files = [\n",
    "    'synthetic_small_v1_chr-10.bed',\n",
    "    'synthetic_small_v1_chr-10.bim',\n",
    "    'synthetic_small_v1_chr-10.fam',\n",
    "    'synthetic_small_v1_chr-10.sample',\n",
    "    'synthetic_small_v1_chr-11.bed',\n",
    "    'synthetic_small_v1_chr-11.bim',\n",
    "    'synthetic_small_v1_chr-11.fam',\n",
    "    'synthetic_small_v1_chr-11.sample',\n",
    "    'synthetic_small_v1_chr-12.bed',\n",
    "    'synthetic_small_v1_chr-12.bim',\n",
    "    'synthetic_small_v1_chr-12.fam',\n",
    "    'synthetic_small_v1_chr-12.sample',\n",
    "    'synthetic_small_v1_chr-13.bed',\n",
    "    'synthetic_small_v1_chr-13.bim',\n",
    "    'synthetic_small_v1_chr-13.fam',\n",
    "    'synthetic_small_v1_chr-13.sample',\n",
    "    'synthetic_small_v1_chr-14.bed',\n",
    "    'synthetic_small_v1_chr-14.bim',\n",
    "    'synthetic_small_v1_chr-14.fam',\n",
    "    'synthetic_small_v1_chr-14.sample',\n",
    "    'synthetic_small_v1_chr-15.bed',\n",
    "    'synthetic_small_v1_chr-15.bim',\n",
    "    'synthetic_small_v1_chr-15.fam',\n",
    "    'synthetic_small_v1_chr-15.sample',\n",
    "    'synthetic_small_v1_chr-16.bed',\n",
    "    'synthetic_small_v1_chr-16.bim',\n",
    "    'synthetic_small_v1_chr-16.fam',\n",
    "    'synthetic_small_v1_chr-16.sample',\n",
    "    'synthetic_small_v1_chr-17.bed',\n",
    "    'synthetic_small_v1_chr-17.bim',\n",
    "    'synthetic_small_v1_chr-17.fam',\n",
    "    'synthetic_small_v1_chr-17.sample',\n",
    "    'synthetic_small_v1_chr-18.bed',\n",
    "    'synthetic_small_v1_chr-18.bim',\n",
    "    'synthetic_small_v1_chr-18.fam',\n",
    "    'synthetic_small_v1_chr-18.sample',\n",
    "    'synthetic_small_v1_chr-19.bed',\n",
    "    'synthetic_small_v1_chr-19.bim',\n",
    "    'synthetic_small_v1_chr-19.fam',\n",
    "    'synthetic_small_v1_chr-19.sample',\n",
    "    'synthetic_small_v1_chr-1.bed',\n",
    "    'synthetic_small_v1_chr-1.bim',\n",
    "    'synthetic_small_v1_chr-1.fam',\n",
    "    'synthetic_small_v1_chr-1.sample',\n",
    "    'synthetic_small_v1_chr-20.bed',\n",
    "    'synthetic_small_v1_chr-20.bim',\n",
    "    'synthetic_small_v1_chr-20.fam',\n",
    "    'synthetic_small_v1_chr-20.sample',\n",
    "    'synthetic_small_v1_chr-21.bed',\n",
    "    'synthetic_small_v1_chr-21.bim',\n",
    "    'synthetic_small_v1_chr-21.fam',\n",
    "    'synthetic_small_v1_chr-21.sample',\n",
    "    'synthetic_small_v1_chr-22.bed',\n",
    "    'synthetic_small_v1_chr-22.bim',\n",
    "    'synthetic_small_v1_chr-22.fam',\n",
    "    'synthetic_small_v1_chr-22.sample',\n",
    "    'synthetic_small_v1_chr-2.bed',\n",
    "    'synthetic_small_v1_chr-2.bim',\n",
    "    'synthetic_small_v1_chr-2.fam',\n",
    "    'synthetic_small_v1_chr-2.sample',\n",
    "    'synthetic_small_v1_chr-3.bed',\n",
    "    'synthetic_small_v1_chr-3.bim',\n",
    "    'synthetic_small_v1_chr-3.fam',\n",
    "    'synthetic_small_v1_chr-3.sample',\n",
    "    'synthetic_small_v1_chr-4.bed',\n",
    "    'synthetic_small_v1_chr-4.bim',\n",
    "    'synthetic_small_v1_chr-4.fam',\n",
    "    'synthetic_small_v1_chr-4.sample',\n",
    "    'synthetic_small_v1_chr-5.bed',\n",
    "    'synthetic_small_v1_chr-5.bim',\n",
    "    'synthetic_small_v1_chr-5.fam',\n",
    "    'synthetic_small_v1_chr-5.sample',\n",
    "    'synthetic_small_v1_chr-6.bed',\n",
    "    'synthetic_small_v1_chr-6.bim',\n",
    "    'synthetic_small_v1_chr-6.fam',\n",
    "    'synthetic_small_v1_chr-6.sample',\n",
    "    'synthetic_small_v1_chr-7.bed',\n",
    "    'synthetic_small_v1_chr-7.bim',\n",
    "    'synthetic_small_v1_chr-7.fam',\n",
    "    'synthetic_small_v1_chr-7.sample',\n",
    "    'synthetic_small_v1_chr-8.bed',\n",
    "    'synthetic_small_v1_chr-8.bim',\n",
    "    'synthetic_small_v1_chr-8.fam',\n",
    "    'synthetic_small_v1_chr-8.sample',\n",
    "    'synthetic_small_v1_chr-9.bed',\n",
    "    'synthetic_small_v1_chr-9.bim',\n",
    "    'synthetic_small_v1_chr-9.fam',\n",
    "    'synthetic_small_v1_chr-9.sample',\n",
    "    'synthetic_small_v1.pheno1',\n",
    "    'synthetic_small_v1.pheno2',\n",
    "    'synthetic_small_v1.pheno3',\n",
    "    'synthetic_small_v1.pheno4',\n",
    "    'synthetic_small_v1.pheno5',\n",
    "    'synthetic_small_v1.pheno6',\n",
    "    'synthetic_small_v1.pheno7',\n",
    "    'synthetic_small_v1.pheno8',\n",
    "    'synthetic_small_v1.pheno9',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a connection to GCS\n",
    "project = 'sgkit-on-coiled'\n",
    "token_path = '/home/codespace/.config/gcloud/application_default_credentials.json'\n",
    "gcs = gcsfs.GCSFileSystem(project=project, token=token_path)\n",
    "#gcs.ls('hapnest/example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file over FTP to local, upload to GCS, delete local\n",
    "# Cell requires: hapnest_example_files, gcs\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "for f in hapnest_example_files:\n",
    "    print('Downloading', f)\n",
    "    urllib.request.urlretrieve(hapnest_ftp_base+f, f)\n",
    "    gcs.put(f, 'hapnest/example/'+f)\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some analysis on the Hapnest example data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An overview of the data\n",
    "- [HAPNEST BioStudies page](https://www.ebi.ac.uk/biostudies/studies/S-BSST936)\n",
    "- [PLINK File format reference](https://www.cog-genomics.org/plink/1.9/formats)\n",
    "\n",
    "- `.bed` files are binary files that contain genotype data.\n",
    "- [`.bim`](https://www.cog-genomics.org/plink/1.9/formats#bim) files are no header TSVs with information about the SNPs in the data.\n",
    "- [`.fam`](https://www.cog-genomics.org/plink/1.9/formats#fam) files are no header TSVs with information about the individuals in the data.\n",
    "- `.sample` has the ancestry group of each individual.\n",
    "- `.phenoX` files are TSVs with headers Sample, GenoEff, CovarEff, EnvEff, Phenotype(liability), Phenotype(binary). Liability is defined in the supplementary materials to be the sum of the covariate effects and the genetic effects. The binary phenotype is defined as 1 if the liability is greater than 0 and 0 otherwise (I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyData\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "xr.set_options(display_expand_attrs=False, display_expand_data_vars=True);\n",
    "\n",
    "# Google Cloud\n",
    "import gcsfs\n",
    "\n",
    "# sgkit\n",
    "import sgkit as sg\n",
    "from sgkit.io import plink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_base = 'synthetic_small_v1_chr-1'\n",
    "\n",
    "# Files to get from GCS\n",
    "files = [\n",
    "    file_base + '.bed',\n",
    "    file_base + '.bim',\n",
    "    file_base + '.fam',\n",
    "]\n",
    "\n",
    "# Local directory to put them in\n",
    "data_dir = '../data/'\n",
    "local_dir = data_dir + file_base\n",
    "\n",
    "local_plink_path = local_dir+'/'+file_base # read_plink adds .bed, .bim, .fam\n",
    "remote_zarr_path = 'gs://hapnest/example/'+file_base+'.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move PLINK files from GCS to the local fs \n",
    "for f in files:\n",
    "    print('Downloading', f)\n",
    "    gcs.get('hapnest/example/'+f, local_dir+'/'+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local PLINK files and write them as Zarr file to GCS\n",
    "chr1 = plink.read_plink(path=local_plink_path)\n",
    "chr1.to_zarr(gcs.get_mapper(remote_zarr_path), mode='w') # do I need get_mapper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sg.load_dataset(remote_zarr_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"variant_contig_name\"] = ds.contig_id[ds.variant_contig]\n",
    "ds2 = ds.set_index({\"variants\": (\"variant_contig_name\", \"variant_position\", \"variant_id\")})\n",
    "sg.display_genotypes(ds2, max_variants=10, max_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_variant = ds[[v for v in ds.data_vars if v.startswith(\"variant_\")]].to_dataframe()\n",
    "df_variant.groupby([\"variant_contig_name\", \"variant_position\", \"variant_id\"]).agg({\"variant_allele\": lambda x: list(x)}).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sample_id[:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_file = 'gs://hapnest/example/'+file_base+'.sample'\n",
    "df = pd.read_csv(ancestry_file, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dask array from df and add to ds as a new data variable called \"sample_ancestry\" with the \"samples\" dimension\n",
    "# Is there a better dtype to use than \"object\"?\n",
    "ancestry_da = da.from_array(df[0].values, chunks=(600,))\n",
    "ancestry_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.assign(sample_ancestry=('samples', ancestry_da))\n",
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
